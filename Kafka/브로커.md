# 브로커

![카프카 시스템 구조](https://velog.velcdn.com/images%2Fjwpark06%2Fpost%2F6434aedf-f349-4cc1-800d-77c8e642eab9%2Fimage.png)


## 브로커 개념 

![카프카 브로커](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmrQ5A%2FbtqC2uxbCkj%2Fdbs2YwdDz86LpAoJmBNa5K%2Fimg.png)

* 브로커 (=카프카 서버)
    * 브로커 내부에 여러 토픽들이 생성될 수 있고, 이러한 토픽들에 의해 생성된 파티션들이 보관하는 데이터에 대해 분산 저장을 한다.
    * 장애 발생 시, 안전하게 데이터를 사용할 수 있도록 도와준다. (데이터 복제, 싱크)

## 데이터 저장, 전송

* 프로듀서로부터 데이터를 전달받으면 카프카 브로커는 프로듀서가 요청한 토픽의 파티션에 데이터를 저장하고, 컨슈머가 데이터를 요청하면 파티션에 저장된 데이터를 전달한다.

![데이터 저장](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdkKtMU%2FbtqC2szqNDt%2FQowXvRgKUAEzQfz282wa1k%2Fimg.png)

* 브로커는 메시지를 로그(log) 자료구조 형태로 디스크에 저장하고, 로그 자료구조는 새로운 쓰기 작업이 반드시 append-only로 진행된다.<br>
따라서, 쓰기 작업이 끝에서만 이뤄지므로 브로커에 이미 쓰여진 메시지(로그)는 변경이 불가능하지만, 대신 빠른 쓰기 작업이 가능하다.
* 프로듀서로부터 전달 받은 데이터는 파일 시스템에 저장된다.
* 카프카는 `페이지 캐시`를 사용하여 디스크 I/O 속도를 높여서 처리 속도를 높였다. 한 번 읽은 파일의 내용은 메모리의 페이지 캐시 영역에 저장하고, 추후 동일한 파일의 접근이 일어나면 디스크에서 읽지 않고 메모리에서 직접 읽는 방식이다. 

### // TODO. Zero Copy

* Zery Copy - 브로커가 세그먼트로부터 메시지를 읽고, 이를 네트워크로 전달하는 과정에서 Context Switch 가 없도록 한다.
* 디스크로부터 네트워크로 데이터를 보낼 때 과잉 데이터 복사가 없도록 최적화되어있다. (https://dev-alxndr.tistory.com/46)

## 로그 세그먼트 / 데이터 파일 관리

* 오직 브로커만이 데이터를 삭제할 수 있다.
* 데이터 삭제는 파일 단위로 이루어진다. (log segment)
* 로그 세그먼트에는 다수의 데이터가 들어 있기 때문에 일반적인 DB 처럼 특정 데이터를 선별해서 삭제할 수 없다.
* 데이터를 삭제하지 않고, 메시지 키를 기준으로 오래된 데이터를 압축하는 정책을 가져갈 수도 있다.

### 로그 세그먼트 컴팩션

* 카프카에서 로그 세그먼트를 컴팩션하면 메시지 키값을 기준으로 마지막 데이터만 보관한다.
* 예) __consumer_offset 토픽에 해당 컨슈머 그룹이 어디까지 읽었는지를 키(컨슈머 그룹명, 토픽명)과 밸류(오프셋 커밋 정보) 형태로 메시지가 저장된다.
* 로그 컴팩션은 메시지의 키값을 기준으로 과거 정보는 중요하지 않고, 가장 마지막 값이 필요한 경우에 사용한다.
* 로그 컴팩션을 사용하려면, 카프카로 메시지를 전송 시 키도 필수값으로 전송해야 한다.

![로그 컴팩션 과정](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcChVQp%2FbtrAqsnTQSt%2FacoHktgzQ5KKhIr4Lr1M01%2Fimg.png)

* 장애 복구 시 전체 로그를 복구하지 않고, 메시지의 키를 기준으로 최신의 상태만 복구할 수 있어 복구 시간을 줄일 수 있다. 다만, 키값을 기준으로 최종값만 필요한 업무에 적용하는 것이 바람직하다.

## 데이터 복제, 싱크

* 클러스터로 묶인 브로커 중 일부에 장애가 발생하더라도 데이터를 유실하지 않고 안전하게 사용할 수 있도록 복제를 한다.
* 카프카의 데이터 복제는 파티션 단위로 이루어진다. (실제로 복제되는 것은 토픽이 아니라, 토픽을 구성하는 각각의 파티션들이다!)
* 리더와 팔로워
    * 리더: 프로듀서 또는 컨슈머와 직접 통신하는 파티션
    * 팔로워: 나머지 복제 데이터를 가지고 있는 파티션
    * 모든 읽기와 쓰기는 리더를 통해서만 가능하다. 즉, 프로듀서는 모든 리플리케이션이 아닌 리더에게만 메시지를 전송하고, 컨슈머는 오직 리더로부터 메시지를 가져온다.
    * 팔로워는 리더에 이슈가 있을 경우를 대비해 항상 준비/복제 동작을 한다.
* `복제`: 팔로워 파티션들이 리더 파티션의 오프셋을 확인하여 현재 자신이 가지고 있는 오프셋과 차이가 나는 경우 리더 파티션으로부터 데이터를 가져와서 자신의 파티션에 저장하는 과정이다. 
* 리플리케이션 팩터 수가 커지면 안정성은 높아지지만, 반대로 브로커 리소스는 많이 사용하게 되므로 최대한 효율적으로 사용해야 한다.
    * 테스트/개발 환경 : 1
    * 운영 환경의 로그성 메시지 : 2 (약간의 유실 허용)
    * 운영 환경 : 3 (유실 허용하지 않음)

### 복제 내부 동작

#### # 리더와 팔로워 간 복제 동작이 어떻게 이루어지는가?

* 리더와 팔로워는 ISR(InSyncReplica)라는 논리적 그룹에 묶여 있고, 여기에 속한 팔로워만 새로운 리더의 자격을 가진다.
* ISR 내의 팔로워들은 리더와의 데이터 일치를 유지하기 위해 지속적으로 리더의 데이터를 따라가고, 리더는 ISR 내 모든 팔로워가 메시지를 받을 때까지 기다린다.

#### # 리더는 팔로워가 리플리케이션 동작을 잘 수행하고 있는지도 판단한다.

* 팔로워가 특정 주기의 시간만큼 복제 요청을 하지 않는다? → 해당 팔로워가 문제가 발생했다고 판단하여 ISR 그룹에서 추방한다.
* ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 "커밋" 되었다는 표시를 해준다. (=리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 저장했다.)
* ___메시지의 일관성을 위해, 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있다.___ (커밋되지 않은 메시지를 컨슈머가 읽어간다면, 동일한 토픽의 파티션에서 컨슘했음에도 메시지가 일치하지 않는 현상이 발생할 수 있다.)
* 커밋된 위치는 로컬 디스크의 replication-offset-checkpoint 라는 파일에 저장한다. 

#### # 리더와 팔로워의 단계별 리플리케이션 동작

* 리더 팔로워 간의 리플리케이션 동작을 처리할 때 서로의 통신을 최소화하도록 설계하여 리더의 부하를 줄인다. 심지어 ACK 을 주고받는 통신도 제거했다.
* 리플리케이션 과정
    * ![리플리케이션 과정1](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fd0dKy4%2FbtrAs8IOZbc%2Fxrk5VrftPU4ZDkGguj8ONk%2Fimg.png)
        * 0번 오프셋에 대한 복제를 마친 팔로워들은 리더에게 1번 오프셋에 대한 복제를 요청하고, 리더는 오프셋 0에 대한 커밋 표시를 한다. 만약 팔로워가 0번 오프셋에 대한 복제를 성공하지 못했다면, 팔로워는 1번이 아니라 0번 오프셋에 대한 복제 요청을 하게 된다.
        * 즉, _리더는 팔로워들이 보내는 복제 요청의 오프셋을 보고, 팔로워들이 어느 위치의 오프셋까지 복제를 성공했는지를 인지할 수 있다._
        * 팔로워들로부터 1번 오프셋 메시지에 대한 복제 요청을 받은 리더는 응답에 0번 오프셋 message1 메시지가 커밋되었다는 내용도 함께 전달한다.
    * ![리플리케이션 과정2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbe2J0n%2FbtrArnTTDHV%2FEP8wk0p5ILfNFaIdWOwk2k%2Fimg.png)
        * 리더의 응답을 받은 모든 팔로워는 0번 오프셋 메시지가 커밋되었다는 것을 인지하고, 리더와 동일하게 커밋을 표시한다. 이후 1번 오프셋 메시지를 복제하는 과정이 이어진다.
* ACK 통신 단계를 제거하여, 메시지를 주고받는 기능에 집중하도록 한다.
* 리더가 push하는 방식이 아니라 팔로워가 pull 하는 방식으로 동작하여 리더의 부하를 줄여준다.

### // TODO. 리더에포크와 복구

* https://zeroco.tistory.com/106

## Controller

* 클러스터의 여러 브로커 중 하나의 브로커가 컨트롤러의 역할을 하게 된다.
* 브로커들의 생존 여부를 체크하여 클러스터 내의 브로커가 장애 발생으로 사용할 수 없는 경우, 장애가 발생한 브로커의 토픽에 있는 리더 파티션을 같은 클러스터 내의 정상 동작하는 다른 브로커에게 토픽의 리더 파티션 지위를 재분배한다. (leader election)
* 만약, 컨트롤러 역할을 하는 브로커에 장애가 생기면 다른 브로커가 컨트롤러 역할을 한다.


## Coordinator

* 클러스터의 여러 브로커 중 한 대는 코디네이터의 역할을 하게 된다.
* 코디네이터는 컨슈머 그룹의 상태를 체크하여 컨슈머 그룹 내의 컨슈머가 장애가 발생하여 매칭된 파티션의 데이터를 cosume 할 수 없는 경우, 장애가 발생한 컨슈머에게 매칭된 파티션을 정상 동작하는 다른 컨슈머에게 재할당한다. (rebalance)


## Reference

* https://velog.io/@jwpark06/Kafka-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B5%AC%EC%A1%B0-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0
* https://always-kimkim.tistory.com/entry/kafka101-broker
* https://zeroco.tistory.com/106